{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8455199-282b-4c01-8315-101359b99159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] GPU: Tesla V100-SXM2-32GB | total ~31.7 GiB | use fraction≈0.45\n",
      "[TEST] Tensor allocation ramp: target ≈ 4.76 GiB (blocks of 256 MB)\n",
      "  - allocated ~4.75 GiB (done)\n",
      "[TEST] Model batch ramp: start=16, up to 128 (no AMP)\n",
      "  - OK @ batch=32\n",
      "===== OOM SANITY SUMMARY =====\n",
      "{\n",
      "  \"gpu_name\": \"Tesla V100-SXM2-32GB\",\n",
      "  \"total_gb\": 31.73,\n",
      "  \"fraction_hint\": 0.45,\n",
      "  \"tensor_ramp_ok\": true,\n",
      "  \"tensor_ramp_target_gb\": 4.76,\n",
      "  \"model_ramp_ok\": true,\n",
      "  \"model_max_safe_bs\": 32\n",
      "}\n",
      "\n",
      "[PASS] 동시 실행 환경에서 보수적 수준에서는 OOM 없이 동작했습니다.\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, math\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "REPORT = {\"phase\": []}\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except Exception as e:\n",
    "    print(\"[FAIL] torch import 실패:\", e)\n",
    "    raise SystemExit\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"[FAIL] CUDA 사용 불가 — GPU가 보이지 않습니다.\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ---------- 파라미터(필요시만 바꾸세요) ----------\n",
    "FRACTION = float(os.getenv(\"TORCH_PER_PROCESS_FRACTION\", \"0.45\"))  # 각자 0.40~0.50 권장\n",
    "ALLOC_TARGET_RATIO = 0.15   # 전체 VRAM의 15% 정도 텐서로 점진 할당(보수적)\n",
    "BLOCK_MB = 256              # 한 번에 할당하는 블록 크기(MB). 너무 크게 잡으면 급격 OOM 가능\n",
    "BATCH_START = 16            # 소형 합성 모델 배치 램프 시작값\n",
    "BATCH_MAX = 128             # 램프 상한\n",
    "IMG_SIZE = 224              # 224x224 합성 이미지\n",
    "# ----------------------------------------------------\n",
    "\n",
    "dev = torch.device(\"cuda\")\n",
    "idx = torch.cuda.current_device()\n",
    "props = torch.cuda.get_device_properties(idx)\n",
    "total = props.total_memory\n",
    "total_gb = total / (1024**3)\n",
    "name = torch.cuda.get_device_name(idx)\n",
    "\n",
    "# (1) 소프트 상한(권고) 적용 — \"하드 리밋\"은 아니지만 충돌 완화에 도움\n",
    "try:\n",
    "    torch.cuda.set_per_process_memory_fraction(FRACTION, device=idx)\n",
    "    REPORT[\"phase\"].append(f\"soft_cap_set:{FRACTION}\")\n",
    "except Exception as e:\n",
    "    REPORT[\"phase\"].append(f\"soft_cap_failed:{e}\")\n",
    "\n",
    "# (2) 기본 정보 출력\n",
    "print(f\"[INFO] GPU: {name} | total ~{total_gb:.1f} GiB | use fraction≈{FRACTION}\")\n",
    "\n",
    "# (3) 소량 워밍업\n",
    "torch.cuda.empty_cache(); torch.cuda.synchronize()\n",
    "_ = torch.empty((1024,1024), device=dev); torch.cuda.synchronize()\n",
    "del _; torch.cuda.empty_cache(); torch.cuda.synchronize()\n",
    "\n",
    "# (4) 점진 텐서 할당 테스트 — 목표치까지 BLOCK 단위로 할당\n",
    "target_bytes = int(total * ALLOC_TARGET_RATIO)  # e.g. 15%\n",
    "block_bytes  = BLOCK_MB * 1024**2\n",
    "alloc_list = []\n",
    "allocated = 0\n",
    "oom_during_tensor = False\n",
    "\n",
    "print(f\"[TEST] Tensor allocation ramp: target ≈ {target_bytes/1024**3:.2f} GiB \"\n",
    "      f\"(blocks of {BLOCK_MB} MB)\")\n",
    "try:\n",
    "    while allocated + block_bytes <= target_bytes:\n",
    "        # float32 기준으로 개수 계산\n",
    "        n = block_bytes // 4\n",
    "        t = torch.empty(n, dtype=torch.float32, device=dev)\n",
    "        alloc_list.append(t)\n",
    "        allocated += block_bytes\n",
    "        if len(alloc_list) % 8 == 0:\n",
    "            torch.cuda.synchronize()\n",
    "            cur = torch.cuda.memory_allocated(idx) / (1024**3)\n",
    "            print(f\"  - allocated ~{cur:.2f} GiB\", end=\"\\r\")\n",
    "    torch.cuda.synchronize()\n",
    "    cur = torch.cuda.memory_allocated(idx) / (1024**3)\n",
    "    print(f\"  - allocated ~{cur:.2f} GiB (done)\")\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        oom_during_tensor = True\n",
    "        print(\"\\n[WARN] OOM during tensor ramp — 동시 사용 충돌 가능성\")\n",
    "    else:\n",
    "        raise\n",
    "finally:\n",
    "    for t in alloc_list: del t\n",
    "    torch.cuda.empty_cache(); torch.cuda.synchronize()\n",
    "\n",
    "REPORT[\"tensor_ramp_ok\"] = not oom_during_tensor\n",
    "REPORT[\"tensor_ramp_target_gb\"] = round(target_bytes/(1024**3), 2)\n",
    "\n",
    "# (5) 합성 소형 모델로 배치 램프(AMP 미사용; 일반적 worst-case에 가깝게)\n",
    "#     모델이 작아서 속도 빨라요. 배치를 키워가며 OOM을 보는 테스트.\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool  = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc    = nn.Linear(128, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x).view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = TinyNet().to(dev)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def try_batch(bs):\n",
    "    x = torch.randn(bs, 3, IMG_SIZE, IMG_SIZE, device=dev)\n",
    "    y = torch.randint(0, 1000, (bs,), device=dev)\n",
    "    out = model(x)\n",
    "    loss = loss_fn(out, y)\n",
    "    loss.backward()\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    # 메모리 회수\n",
    "    del x, y, out, loss\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "max_safe_bs = 0\n",
    "oom_during_model = False\n",
    "bs = BATCH_START\n",
    "print(f\"[TEST] Model batch ramp: start={BATCH_START}, up to {BATCH_MAX} (no AMP)\")\n",
    "while bs <= BATCH_MAX:\n",
    "    try:\n",
    "        try_batch(bs)\n",
    "        max_safe_bs = bs\n",
    "        print(f\"  - OK @ batch={bs}\", end=\"\\r\")\n",
    "        bs *= 2 if bs < 32 else bs + 16  # 초반엔 빠르게, 이후엔 완만히 증가\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            oom_during_model = True\n",
    "            print(f\"\\n[WARN] OOM @ batch={bs}\")\n",
    "            break\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "REPORT[\"model_ramp_ok\"] = not oom_during_model\n",
    "REPORT[\"model_max_safe_bs\"] = int(max_safe_bs)\n",
    "\n",
    "# (6) 요약 리포트\n",
    "summary = {\n",
    "    \"gpu_name\": name,\n",
    "    \"total_gb\": round(total_gb, 2),\n",
    "    \"fraction_hint\": FRACTION,\n",
    "    \"tensor_ramp_ok\": REPORT[\"tensor_ramp_ok\"],\n",
    "    \"tensor_ramp_target_gb\": REPORT[\"tensor_ramp_target_gb\"],\n",
    "    \"model_ramp_ok\": REPORT[\"model_ramp_ok\"],\n",
    "    \"model_max_safe_bs\": REPORT[\"model_max_safe_bs\"],\n",
    "}\n",
    "print(\"\\n===== OOM SANITY SUMMARY =====\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "# (7) 간단 판정\n",
    "if summary[\"tensor_ramp_ok\"] and summary[\"model_ramp_ok\"]:\n",
    "    print(\"\\n[PASS] 동시 실행 환경에서 보수적 수준에서는 OOM 없이 동작했습니다.\")\n",
    "else:\n",
    "    print(\"\\n[ATTN] 동시 실행 시 OOM 징후가 있습니다.\")\n",
    "    print(\" - 배치 크기를 낮추거나 AMP 사용, 또는 다른 사용자의 부하와 시간 분리(순차 실행)를 권장합니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480575d9-b9c7-4aa5-a1aa-4251dc1bd902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
