{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816a77e4-9c6a-4510-ada0-f9f175e5f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "from glob import glob\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d906df9-bbe4-4a3d-aacc-2894f4f6f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습(train) 세트 경로\n",
    "TRAIN_ROOT: Path = Path(\"dataset/traffic_data/training/bbox_tr\")\n",
    "TRAIN_LABEL_DIR: Path = TRAIN_ROOT / \"label_bbox\"\n",
    "TRAIN_IMAGE_DIR: Path = TRAIN_ROOT / \"image_bbox\"\n",
    "\n",
    "# 검증(val) 세트 경로\n",
    "VAL_ROOT: Path = Path(\"dataset/traffic_data/validation/bbox_val\")\n",
    "VAL_LABEL_DIR: Path = VAL_ROOT / \"label_bbox\"\n",
    "VAL_IMAGE_DIR: Path = VAL_ROOT / \"image_bbox\"\n",
    "\n",
    "# 출력 루트\n",
    "OUT_ROOT: Path = Path(\"yolo_transfer\")\n",
    "\n",
    "# 전이학습 3단계 규칙 -> 정규화 없이 값 그대로 비교\n",
    "STAGES: Dict[str, Dict[str, Set[str]]] = {\n",
    "    \"day_data\": {\n",
    "        \"weathers\": set([\"Sunny\"]),\n",
    "        \"times\": set([\"낮\", \"오전\"])\n",
    "    },\n",
    "    \"night_data\": {\n",
    "        \"weathers\": set([\"Sunny\"]),\n",
    "        \"times\": set([\"새벽\", \"오후\"])\n",
    "    },\n",
    "    \"bad_weather_data\": {\n",
    "        \"weathers\": set([\"Cloudy\", \"Foggy\", \"Rainy\", \"Snow\"]),\n",
    "        \"times\": set([\"낮\", \"오전\", \"새벽\", \"오후\"])\n",
    "    }\n",
    "}\n",
    "\n",
    "# 실행 옵션\n",
    "OVERWRITE_LABELS: bool = True\n",
    "COPY_IMAGES: bool = True\n",
    "DRY_RUN: bool = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b10237-d6f6-405c-8a6d-52061bc2dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_camera_index(images_root: Path) -> Dict[str, List[Path]]:\n",
    "    # 패턴 일반화 -> image_bbox/site1/site2/camera_id/ 까지 모든 디렉터리 순회\n",
    "    cam_index: Dict[str, List[Path]] = {}\n",
    "    if not images_root.exists():\n",
    "        return cam_index\n",
    "    site1: Path\n",
    "    for site1 in images_root.iterdir():\n",
    "        if not site1.is_dir():\n",
    "            continue\n",
    "        site2: Path\n",
    "        for site2 in site1.iterdir():\n",
    "            if not site2.is_dir():\n",
    "                continue\n",
    "            cam_dir: Path\n",
    "            for cam_dir in site2.iterdir():\n",
    "                if not cam_dir.is_dir():\n",
    "                    continue\n",
    "                cam_id: str = cam_dir.name\n",
    "                cam_index.setdefault(cam_id, []).append(cam_dir)\n",
    "    return cam_index\n",
    "\n",
    "def find_image_path_from_index(cam_index: Dict[str, List[Path]], camera_id: str, filename: str) -> Optional[Path]:\n",
    "    dirs: List[Path] = cam_index.get(camera_id, [])\n",
    "    d: Path\n",
    "    for d in dirs:\n",
    "        candidate: Path = d / filename\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "def xyxy_to_yolo(x1: float, y1: float, x2: float, y2: float, w: int, h: int) -> Tuple[float, float, float, float]:\n",
    "    x1c: float = float(min(max(x1, 0.0), float(w)))\n",
    "    y1c: float = float(min(max(y1, 0.0), float(h)))\n",
    "    x2c: float = float(min(max(x2, 0.0), float(w)))\n",
    "    y2c: float = float(min(max(y2, 0.0), float(h)))\n",
    "\n",
    "    xmin: float = float(min(x1c, x2c))\n",
    "    xmax: float = float(max(x1c, x2c))\n",
    "    ymin: float = float(min(y1c, y2c))\n",
    "    ymax: float = float(max(y1c, y2c))\n",
    "\n",
    "    bw: float = float(max(xmax - xmin, 1e-6))\n",
    "    bh: float = float(max(ymax - ymin, 1e-6))\n",
    "\n",
    "    cx: float = float(xmin + bw / 2.0)\n",
    "    cy: float = float(ymin + bh / 2.0)\n",
    "\n",
    "    x_norm: float = float(cx / float(w))\n",
    "    y_norm: float = float(cy / float(h))\n",
    "    w_norm: float = float(bw / float(w))\n",
    "    h_norm: float = float(bh / float(h))\n",
    "    return x_norm, y_norm, w_norm, h_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598f3598-5162-4448-a67b-fd62e005b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_stage(weather: str, time_space: str, stages: Dict[str, Dict[str, Set[str]]]) -> Optional[str]:\n",
    "    stage_name: str\n",
    "    rule: Dict[str, Set[str]]\n",
    "    for stage_name, rule in stages.items():\n",
    "        if weather in rule[\"weathers\"] and time_space in rule[\"times\"]:\n",
    "            return stage_name\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb875796-1ead-48df-a638-0d92f39ffcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_coco_json_fixed_subset(\n",
    "    json_path: Path,\n",
    "    cam_index: Dict[str, List[Path]],\n",
    "    out_root: Path,\n",
    "    stages: Dict[str, Dict[str, Set[str]]],\n",
    "    id_to_idx: Dict[int, int],\n",
    "    id_to_name: Dict[int, str],\n",
    "    overwrite_labels: bool,\n",
    "    copy_images: bool,\n",
    "    dry_run: bool,\n",
    "    subset_name: str\n",
    ") -> Dict[str, int]:\n",
    "    counts: Dict[str, int] = {\"copied\": 0, \"labeled\": 0, \"missing_image\": 0, \"skipped_stage\": 0}\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        coco_any: Any = json.load(f)\n",
    "\n",
    "    meta_list: List[Dict[str, Any]] = list(coco_any.get(\"meta\", []))  # type: ignore\n",
    "    meta_by_id: Dict[int, Dict[str, Any]] = {int(m.get(\"id\", -1)): m for m in meta_list}\n",
    "\n",
    "    images: List[Dict[str, Any]] = list(coco_any.get(\"images\", []))  # type: ignore\n",
    "    anns: List[Dict[str, Any]] = list(coco_any.get(\"annotations\", []))  # type: ignore\n",
    "\n",
    "    anns_by_image: Dict[int, List[Dict[str, Any]]] = {}\n",
    "    ann: Dict[str, Any]\n",
    "    for ann in anns:\n",
    "        img_id_val: int = int(ann.get(\"image_id\", -1))\n",
    "        anns_by_image.setdefault(img_id_val, []).append(ann)\n",
    "\n",
    "    stage_key: str\n",
    "    for stage_key in stages.keys():\n",
    "        ensure_dir(out_root / stage_key / subset_name / \"images\")\n",
    "        ensure_dir(out_root / stage_key / subset_name / \"labels\")\n",
    "\n",
    "    img: Dict[str, Any]\n",
    "    for img in images:\n",
    "        img_id: int = int(img.get(\"id\", -1))\n",
    "        meta_id: int = int(img.get(\"meta_id\", -1))\n",
    "        file_name: str = str(img.get(\"file_name\", \"\"))  # 예: \"BC2000201/xxxx.jpg\"\n",
    "\n",
    "        parts: List[str] = file_name.split(\"/\")\n",
    "        camera_id: str = parts[0] if len(parts) >= 2 else \"\"\n",
    "        fname: str = parts[-1]\n",
    "\n",
    "        meta: Dict[str, Any] = meta_by_id.get(meta_id, {})\n",
    "        weather: str = str(meta.get(\"weather\", \"\")).strip()\n",
    "        time_space: str = str(meta.get(\"time_space\", \"\")).strip()\n",
    "\n",
    "        stage: Optional[str] = decide_stage(weather, time_space, stages)\n",
    "        if stage is None:\n",
    "            counts[\"skipped_stage\"] += 1\n",
    "            continue\n",
    "\n",
    "        src_path_opt: Optional[Path] = find_image_path_from_index(cam_index, camera_id, fname)\n",
    "        if src_path_opt is None or not src_path_opt.exists():\n",
    "            counts[\"missing_image\"] += 1\n",
    "            continue\n",
    "        src_path: Path = src_path_opt\n",
    "\n",
    "        dst_img_dir: Path = out_root / stage / subset_name / \"images\" / camera_id\n",
    "        dst_lbl_dir: Path = out_root / stage / subset_name / \"labels\" / camera_id\n",
    "        ensure_dir(dst_img_dir)\n",
    "        ensure_dir(dst_lbl_dir)\n",
    "\n",
    "        dst_img_path: Path = dst_img_dir / fname\n",
    "        dst_lbl_path: Path = dst_lbl_dir / (Path(fname).stem + \".txt\")\n",
    "\n",
    "        with Image.open(src_path) as im:\n",
    "            size_tuple: Tuple[int, int] = im.size\n",
    "        w: int = int(size_tuple[0])\n",
    "        h: int = int(size_tuple[1])\n",
    "\n",
    "        lines: List[str] = []\n",
    "        img_anns: List[Dict[str, Any]] = anns_by_image.get(img_id, [])\n",
    "        ann_item: Dict[str, Any]\n",
    "        for ann_item in img_anns:\n",
    "            bboxes: List[List[float]] = [list(map(float, b)) for b in ann_item.get(\"bbox\", [])]  # type: ignore\n",
    "            cat_ids: List[int] = [int(c) for c in ann_item.get(\"category_id\", [])]  # type: ignore\n",
    "            idx: int\n",
    "            for idx in range(0, min(len(bboxes), len(cat_ids))):\n",
    "                b: List[float] = bboxes[idx]\n",
    "                c: int = cat_ids[idx]\n",
    "                if len(b) < 4:\n",
    "                    continue\n",
    "                x1: float = float(b[0]); y1: float = float(b[1]); x2: float = float(b[2]); y2: float = float(b[3])\n",
    "                cxn: float; cyn: float; wn: float; hn: float\n",
    "                cxn, cyn, wn, hn = xyxy_to_yolo(x1, y1, x2, y2, w, h)\n",
    "                cls_idx: int = int(id_to_idx.get(c, 0))\n",
    "                line: str = f\"{cls_idx} {cxn:.6f} {cyn:.6f} {wn:.6f} {hn:.6f}\"\n",
    "                lines.append(line)\n",
    "\n",
    "        if not dry_run:\n",
    "            if OVERWRITE_LABELS or not dst_lbl_path.exists():\n",
    "                with open(dst_lbl_path, \"w\", encoding=\"utf-8\") as lf:\n",
    "                    content: str = \"\\n\".join(lines)\n",
    "                    lf.write(content)\n",
    "            if COPY_IMAGES and not dst_img_path.exists():\n",
    "                shutil.copy2(src_path, dst_img_path)\n",
    "            counts[\"labeled\"] += 1\n",
    "            if COPY_IMAGES:\n",
    "                counts[\"copied\"] += 1\n",
    "        else:\n",
    "            counts[\"labeled\"] += 1\n",
    "            if COPY_IMAGES:\n",
    "                counts[\"copied\"] += 1\n",
    "\n",
    "    # data.yaml은 run_pipeline에서 한 번만 생성\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076cd118-b4b4-49a6-8657-f70465d0b5b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train json files:   0%|          | 0/72 [00:00<?, ?file/s]\n",
      "train json files:   1%|▏         | 1/72 [00:06<07:56,  6.71s/file]\n",
      "train json files:   1%|▏         | 1/72 [00:06<07:56,  6.71s/file, labeled=2404, missing=0, skipped=0]\n",
      "train json files:   3%|▎         | 2/72 [00:25<16:12, 13.89s/file, labeled=2404, missing=0, skipped=0]\n",
      "train json files:   4%|▍         | 3/72 [00:30<11:16,  9.80s/file, labeled=4808, missing=0, skipped=0]\n",
      "train json files:   6%|▌         | 4/72 [00:44<12:49, 11.32s/file, labeled=6590, missing=0, skipped=0]\n",
      "train json files:   7%|▋         | 5/72 [00:45<08:36,  7.70s/file, labeled=8372, missing=0, skipped=0]\n",
      "train json files:   8%|▊         | 6/72 [00:49<07:07,  6.47s/file, labeled=8865, missing=0, skipped=0]\n",
      "train json files:  10%|▉         | 7/72 [00:50<05:08,  4.75s/file, labeled=10391, missing=0, skipped=0]\n",
      "train json files:  10%|▉         | 7/72 [00:50<05:08,  4.75s/file, labeled=10818, missing=0, skipped=0]\n",
      "train json files:  11%|█         | 8/72 [01:10<10:09,  9.52s/file, labeled=10818, missing=0, skipped=0]\n",
      "train json files:  12%|█▎        | 9/72 [01:22<10:55, 10.40s/file, labeled=17882, missing=0, skipped=0]\n",
      "train json files:  12%|█▎        | 9/72 [01:22<10:55, 10.40s/file, labeled=19388, missing=0, skipped=0]\n",
      "train json files:  14%|█▍        | 10/72 [01:45<14:31, 14.06s/file, labeled=19388, missing=0, skipped=0]\n",
      "train json files:  14%|█▍        | 10/72 [01:45<14:31, 14.06s/file, labeled=21409, missing=0, skipped=0]\n",
      "train json files:  15%|█▌        | 11/72 [02:19<20:44, 20.40s/file, labeled=21409, missing=0, skipped=0]\n",
      "train json files:  17%|█▋        | 12/72 [02:22<14:59, 15.00s/file, labeled=25046, missing=0, skipped=0]\n",
      "train json files:  18%|█▊        | 13/72 [02:27<11:38, 11.84s/file, labeled=25346, missing=0, skipped=0]\n",
      "train json files:  19%|█▉        | 14/72 [02:34<10:08, 10.50s/file, labeled=26959, missing=0, skipped=0]\n",
      "train json files:  21%|██        | 15/72 [02:40<08:34,  9.02s/file, labeled=29668, missing=0, skipped=0]\n",
      "train json files:  22%|██▏       | 16/72 [02:48<08:13,  8.82s/file, labeled=31596, missing=0, skipped=0]\n",
      "train json files:  24%|██▎       | 17/72 [02:54<07:15,  7.92s/file, labeled=34628, missing=0, skipped=0]\n",
      "train json files:  25%|██▌       | 18/72 [03:07<08:29,  9.43s/file, labeled=36674, missing=0, skipped=0]\n",
      "train json files:  26%|██▋       | 19/72 [03:12<07:19,  8.30s/file, labeled=41084, missing=0, skipped=0]\n",
      "train json files:  28%|██▊       | 20/72 [03:22<07:35,  8.76s/file, labeled=42945, missing=0, skipped=0]\n",
      "train json files:  29%|██▉       | 21/72 [03:29<06:49,  8.02s/file, labeled=46126, missing=0, skipped=0]\n",
      "train json files:  33%|███▎      | 24/72 [03:40<04:26,  5.55s/file, labeled=48162, missing=0, skipped=0]\n",
      "train json files:  35%|███▍      | 25/72 [03:46<04:33,  5.82s/file, labeled=51718, missing=0, skipped=0]\n",
      "train json files:  36%|███▌      | 26/72 [03:55<05:01,  6.56s/file, labeled=53941, missing=0, skipped=0]\n",
      "train json files:  38%|███▊      | 27/72 [04:01<04:46,  6.37s/file, labeled=56543, missing=0, skipped=0]\n",
      "train json files:  39%|███▉      | 28/72 [04:07<04:37,  6.31s/file, labeled=58434, missing=0, skipped=0]\n",
      "train json files:  39%|███▉      | 28/72 [04:07<04:37,  6.31s/file, labeled=60465, missing=0, skipped=0]\n",
      "train json files:  40%|████      | 29/72 [04:36<09:15, 12.91s/file, labeled=60465, missing=0, skipped=0]\n",
      "train json files:  40%|████      | 29/72 [04:36<09:15, 12.91s/file, labeled=63339, missing=0, skipped=0]\n",
      "train json files:  42%|████▏     | 30/72 [04:57<10:35, 15.12s/file, labeled=63339, missing=0, skipped=0]\n",
      "train json files:  43%|████▎     | 31/72 [05:00<08:00, 11.71s/file, labeled=69837, missing=0, skipped=0]\n",
      "train json files:  44%|████▍     | 32/72 [05:06<06:41, 10.04s/file, labeled=71062, missing=0, skipped=0]\n",
      "train json files:  46%|████▌     | 33/72 [05:15<06:17,  9.69s/file, labeled=73139, missing=0, skipped=0]\n",
      "train json files:  47%|████▋     | 34/72 [05:27<06:37, 10.45s/file, labeled=76231, missing=0, skipped=0]\n",
      "train json files:  49%|████▊     | 35/72 [05:38<06:23, 10.35s/file, labeled=80164, missing=0, skipped=0]\n",
      "train json files:  50%|█████     | 36/72 [05:41<05:03,  8.42s/file, labeled=83453, missing=0, skipped=0]\n",
      "train json files:  51%|█████▏    | 37/72 [05:50<04:54,  8.40s/file, labeled=84540, missing=0, skipped=0]\n",
      "train json files:  53%|█████▎    | 38/72 [06:02<05:24,  9.53s/file, labeled=85341, missing=1, skipped=0]\n",
      "train json files:  54%|█████▍    | 39/72 [06:19<06:31, 11.87s/file, labeled=86445, missing=1, skipped=0]\n",
      "train json files:  54%|█████▍    | 39/72 [06:19<06:31, 11.87s/file, labeled=88057, missing=1, skipped=0]\n",
      "train json files:  56%|█████▌    | 40/72 [06:44<08:27, 15.86s/file, labeled=88057, missing=1, skipped=0]\n",
      "train json files:  57%|█████▋    | 41/72 [06:47<06:10, 11.96s/file, labeled=90585, missing=1, skipped=0]\n",
      "train json files:  58%|█████▊    | 42/72 [06:58<05:43, 11.46s/file, labeled=91050, missing=1, skipped=0]\n",
      "train json files:  58%|█████▊    | 42/72 [06:58<05:43, 11.46s/file, labeled=93528, missing=1, skipped=0]\n",
      "train json files:  60%|█████▉    | 43/72 [07:10<05:44, 11.87s/file, labeled=93528, missing=1, skipped=0]\n",
      "train json files:  61%|██████    | 44/72 [07:22<05:30, 11.81s/file, labeled=95162, missing=1, skipped=0]\n",
      "train json files:  61%|██████    | 44/72 [07:22<05:30, 11.81s/file, labeled=96579, missing=1, skipped=0]\n",
      "train json files:  62%|██████▎   | 45/72 [07:45<06:44, 14.99s/file, labeled=96579, missing=1, skipped=0]\n",
      "train json files:  64%|██████▍   | 46/72 [07:59<06:22, 14.70s/file, labeled=99152, missing=1, skipped=0]\n",
      "train json files:  64%|██████▍   | 46/72 [07:59<06:22, 14.70s/file, labeled=100963, missing=1, skipped=0]\n",
      "train json files:  65%|██████▌   | 47/72 [08:23<07:23, 17.75s/file, labeled=100963, missing=1, skipped=0]\n",
      "train json files:  65%|██████▌   | 47/72 [08:23<07:23, 17.75s/file, labeled=103845, missing=1, skipped=0]\n",
      "train json files:  67%|██████▋   | 48/72 [08:58<09:07, 22.82s/file, labeled=103845, missing=1, skipped=0]\n",
      "train json files:  67%|██████▋   | 48/72 [08:58<09:07, 22.82s/file, labeled=107423, missing=1, skipped=0]\n",
      "train json files:  68%|██████▊   | 49/72 [09:25<09:11, 23.97s/file, labeled=107423, missing=1, skipped=0]\n",
      "train json files:  68%|██████▊   | 49/72 [09:25<09:11, 23.97s/file, labeled=110109, missing=1, skipped=0]\n",
      "train json files:  69%|██████▉   | 50/72 [10:13<11:27, 31.24s/file, labeled=110109, missing=1, skipped=0]\n",
      "train json files:  69%|██████▉   | 50/72 [10:13<11:27, 31.24s/file, labeled=115025, missing=1, skipped=0]\n",
      "train json files:  71%|███████   | 51/72 [10:58<12:24, 35.45s/file, labeled=115025, missing=1, skipped=0]\n",
      "train json files:  72%|███████▏  | 52/72 [11:07<09:09, 27.46s/file, labeled=119019, missing=1, skipped=0]\n",
      "train json files:  74%|███████▎  | 53/72 [11:10<06:21, 20.09s/file, labeled=120250, missing=1, skipped=0]\n",
      "train json files:  75%|███████▌  | 54/72 [11:17<04:53, 16.28s/file, labeled=120659, missing=1, skipped=0]\n",
      "train json files:  76%|███████▋  | 55/72 [11:22<03:37, 12.77s/file, labeled=121859, missing=1, skipped=0]\n",
      "train json files:  78%|███████▊  | 56/72 [11:28<02:54, 10.92s/file, labeled=122490, missing=1, skipped=0]\n",
      "train json files:  78%|███████▊  | 56/72 [11:28<02:54, 10.92s/file, labeled=123262, missing=1, skipped=0]\n",
      "train json files:  79%|███████▉  | 57/72 [11:42<02:56, 11.74s/file, labeled=123262, missing=1, skipped=0]\n",
      "train json files:  79%|███████▉  | 57/72 [11:42<02:56, 11.74s/file, labeled=124632, missing=1, skipped=0]\n",
      "train json files:  81%|████████  | 58/72 [12:21<04:36, 19.74s/file, labeled=124632, missing=1, skipped=0]\n",
      "train json files:  82%|████████▏ | 59/72 [12:31<03:38, 16.82s/file, labeled=128408, missing=1, skipped=0]\n",
      "train json files:  82%|████████▏ | 59/72 [12:31<03:38, 16.82s/file, labeled=129577, missing=1, skipped=0]\n",
      "train json files:  83%|████████▎ | 60/72 [13:05<04:24, 22.04s/file, labeled=129577, missing=1, skipped=0]\n",
      "train json files:  85%|████████▍ | 61/72 [13:17<03:30, 19.18s/file, labeled=132832, missing=3, skipped=0]\n",
      "train json files:  85%|████████▍ | 61/72 [13:17<03:30, 19.18s/file, labeled=134373, missing=3, skipped=0]\n",
      "train json files:  86%|████████▌ | 62/72 [13:40<03:21, 20.18s/file, labeled=134373, missing=3, skipped=0]\n",
      "train json files:  88%|████████▊ | 63/72 [13:47<02:25, 16.21s/file, labeled=137205, missing=3, skipped=0]\n",
      "train json files:  88%|████████▊ | 63/72 [13:47<02:25, 16.21s/file, labeled=138141, missing=3, skipped=0]\n",
      "train json files:  89%|████████▉ | 64/72 [14:04<02:11, 16.43s/file, labeled=138141, missing=3, skipped=0]\n",
      "train json files:  90%|█████████ | 65/72 [14:16<01:46, 15.18s/file, labeled=140312, missing=3, skipped=0]\n",
      "train json files:  90%|█████████ | 65/72 [14:16<01:46, 15.18s/file, labeled=141640, missing=3, skipped=0]\n",
      "train json files:  92%|█████████▏| 66/72 [14:43<01:52, 18.77s/file, labeled=141640, missing=3, skipped=0]\n",
      "train json files:  93%|█████████▎| 67/72 [14:47<01:11, 14.27s/file, labeled=144983, missing=3, skipped=0]\n",
      "train json files:  94%|█████████▍| 68/72 [14:48<00:41, 10.33s/file, labeled=145519, missing=3, skipped=0]\n",
      "train json files:  94%|█████████▍| 68/72 [14:48<00:41, 10.33s/file, labeled=145677, missing=3, skipped=0]\n",
      "train json files:  96%|█████████▌| 69/72 [15:09<00:40, 13.53s/file, labeled=145677, missing=3, skipped=0]\n",
      "train json files:  97%|█████████▋| 70/72 [15:12<00:20, 10.34s/file, labeled=148695, missing=3, skipped=0]\n",
      "train json files:  99%|█████████▊| 71/72 [15:14<00:07,  7.75s/file, labeled=149079, missing=3, skipped=0]\n",
      "train json files: 100%|██████████| 72/72 [15:20<00:00,  7.41s/file, labeled=149277, missing=3, skipped=0]\n",
      "train images: 100%|██████████| 149887/149887 [15:20<00:00, 162.79img/s]eled=149884, missing=3, skipped=0]\n",
      "train json files: 100%|██████████| 72/72 [15:20<00:00, 12.79s/file, labeled=149884, missing=3, skipped=0]\n",
      "val json files:   0%|          | 0/46 [00:00<?, ?file/s]\n",
      "val json files:   2%|▏         | 1/46 [00:02<01:31,  2.03s/file]\n",
      "val json files:   4%|▍         | 2/46 [00:03<01:18,  1.78s/file, labeled=150107, missing=3, skipped=0]\n",
      "val json files:   7%|▋         | 3/46 [00:04<01:03,  1.47s/file, labeled=150330, missing=3, skipped=0]\n",
      "val json files:   9%|▊         | 4/46 [00:07<01:20,  1.91s/file, labeled=150465, missing=3, skipped=0]\n",
      "val json files:  11%|█         | 5/46 [00:09<01:23,  2.03s/file, labeled=150758, missing=3, skipped=0]\n",
      "val json files:  13%|█▎        | 6/46 [00:13<01:41,  2.54s/file, labeled=150996, missing=3, skipped=0]\n",
      "val json files:  15%|█▌        | 7/46 [00:14<01:24,  2.16s/file, labeled=151393, missing=3, skipped=0]\n",
      "val json files:  17%|█▋        | 8/46 [00:18<01:46,  2.81s/file, labeled=151563, missing=3, skipped=0]\n",
      "val json files:  20%|█▉        | 9/46 [00:23<02:07,  3.46s/file, labeled=152007, missing=3, skipped=0]\n",
      "val json files:  22%|██▏       | 10/46 [00:24<01:41,  2.82s/file, labeled=152477, missing=3, skipped=0]\n",
      "val json files:  24%|██▍       | 11/46 [00:30<02:09,  3.70s/file, labeled=152632, missing=3, skipped=0]\n",
      "val json files:  26%|██▌       | 12/46 [00:37<02:39,  4.69s/file, labeled=153112, missing=3, skipped=0]\n",
      "val json files:  28%|██▊       | 13/46 [00:39<02:10,  3.96s/file, labeled=153910, missing=3, skipped=0]\n",
      "val json files:  30%|███       | 14/46 [00:42<01:56,  3.65s/file, labeled=154196, missing=3, skipped=0]\n",
      "val json files:  33%|███▎      | 15/46 [00:43<01:30,  2.91s/file, labeled=154528, missing=3, skipped=0]\n",
      "val json files:  35%|███▍      | 16/46 [00:49<01:52,  3.74s/file, labeled=154670, missing=3, skipped=0]\n",
      "val json files:  37%|███▋      | 17/46 [01:01<02:57,  6.13s/file, labeled=155258, missing=4, skipped=0]\n",
      "val json files:  37%|███▋      | 17/46 [01:01<02:57,  6.13s/file, labeled=156188, missing=4, skipped=0]\n",
      "val json files:  39%|███▉      | 18/46 [01:21<04:53, 10.48s/file, labeled=156188, missing=4, skipped=0]\n",
      "val json files:  41%|████▏     | 19/46 [01:27<03:59,  8.86s/file, labeled=158167, missing=4, skipped=0]\n",
      "val json files:  43%|████▎     | 20/46 [01:33<03:29,  8.07s/file, labeled=158536, missing=4, skipped=0]\n",
      "val json files:  46%|████▌     | 21/46 [01:35<02:34,  6.19s/file, labeled=159181, missing=4, skipped=0]\n",
      "val json files:  48%|████▊     | 22/46 [01:37<02:01,  5.06s/file, labeled=159406, missing=4, skipped=0]\n",
      "val json files:  50%|█████     | 23/46 [01:38<01:27,  3.79s/file, labeled=159668, missing=4, skipped=0]\n",
      "val json files:  52%|█████▏    | 24/46 [01:41<01:21,  3.70s/file, labeled=159765, missing=4, skipped=0]\n",
      "val json files:  54%|█████▍    | 25/46 [01:43<01:04,  3.05s/file, labeled=160114, missing=4, skipped=0]\n",
      "val json files:  57%|█████▋    | 26/46 [01:45<00:55,  2.77s/file, labeled=160278, missing=4, skipped=0]\n",
      "val json files:  59%|█████▊    | 27/46 [01:48<00:54,  2.85s/file, labeled=160498, missing=4, skipped=0]\n",
      "val json files:  61%|██████    | 28/46 [01:53<01:04,  3.58s/file, labeled=160809, missing=4, skipped=0]\n",
      "val json files:  63%|██████▎   | 29/46 [01:58<01:07,  3.96s/file, labeled=161338, missing=4, skipped=0]\n",
      "val json files:  65%|██████▌   | 30/46 [02:02<01:01,  3.82s/file, labeled=161844, missing=4, skipped=0]\n",
      "val json files:  67%|██████▋   | 31/46 [02:03<00:46,  3.10s/file, labeled=162071, missing=4, skipped=0]\n",
      "val json files:  70%|██████▉   | 32/46 [02:05<00:39,  2.81s/file, labeled=162238, missing=4, skipped=0]\n",
      "val json files:  72%|███████▏  | 33/46 [02:07<00:32,  2.50s/file, labeled=162465, missing=4, skipped=0]\n",
      "val json files:  74%|███████▍  | 34/46 [02:13<00:43,  3.60s/file, labeled=162664, missing=4, skipped=0]\n",
      "val json files:  76%|███████▌  | 35/46 [02:14<00:29,  2.71s/file, labeled=163269, missing=4, skipped=0]\n",
      "val json files:  78%|███████▊  | 36/46 [02:17<00:27,  2.76s/file, labeled=163352, missing=4, skipped=0]\n",
      "val json files:  80%|████████  | 37/46 [02:26<00:42,  4.75s/file, labeled=163635, missing=4, skipped=0]\n",
      "val json files:  83%|████████▎ | 38/46 [02:28<00:30,  3.80s/file, labeled=164546, missing=4, skipped=0]\n",
      "val json files:  85%|████████▍ | 39/46 [02:34<00:32,  4.63s/file, labeled=164729, missing=4, skipped=0]\n",
      "val json files:  87%|████████▋ | 40/46 [02:36<00:23,  3.91s/file, labeled=165376, missing=4, skipped=0]\n",
      "val json files:  89%|████████▉ | 41/46 [02:40<00:18,  3.77s/file, labeled=165613, missing=4, skipped=0]\n",
      "val json files:  91%|█████████▏| 42/46 [02:43<00:13,  3.43s/file, labeled=166038, missing=4, skipped=0]\n",
      "val json files:  93%|█████████▎| 43/46 [02:48<00:12,  4.07s/file, labeled=166316, missing=4, skipped=0]\n",
      "val json files:  96%|█████████▌| 44/46 [02:53<00:08,  4.18s/file, labeled=166948, missing=4, skipped=0]\n",
      "val json files:  98%|█████████▊| 45/46 [03:00<00:05,  5.16s/file, labeled=167433, missing=4, skipped=0]\n",
      "val json files: 100%|██████████| 46/46 [03:01<00:00,  3.80s/file, labeled=168250, missing=4, skipped=0]\n",
      "val images: 100%|██████████| 18435/18435 [03:01<00:00, 101.78img/s]abeled=168318, missing=4, skipped=0]\n",
      "val json files: 100%|██████████| 46/46 [03:01<00:00,  3.94s/file, labeled=168318, missing=4, skipped=0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'copied': 168318, 'labeled': 168318, 'missing_image': 4, 'skipped_stage': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_category_maps(first_json_path: Path) -> Tuple[Dict[int, int], Dict[int, str]]:\n",
    "    with open(first_json_path, \"r\", encoding=\"utf-8\") as f0:\n",
    "        first_any: Any = json.load(f0)\n",
    "    categories: List[Dict[str, Any]] = list(first_any.get(\"categories\", []))  # type: ignore\n",
    "\n",
    "    id_to_idx: Dict[int, int] = {}\n",
    "    id_to_name: Dict[int, str] = {}\n",
    "    cat_obj: Dict[str, Any]\n",
    "    for cat_obj in sorted(categories, key=lambda c: int(c.get(\"id\", 0))):\n",
    "        cid: int = int(cat_obj.get(\"id\", 0))\n",
    "        cname: str = str(cat_obj.get(\"name\", \"\"))\n",
    "        id_to_idx[cid] = cid - 1\n",
    "        id_to_name[cid] = cname\n",
    "    return id_to_idx, id_to_name\n",
    "\n",
    "\n",
    "def _count_images_in_json(json_path: Path) -> int:\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            coco_any: Any = json.load(f)\n",
    "        images_any: Any = coco_any.get(\"images\", [])\n",
    "        images: List[Dict[str, Any]] = list(images_any) if isinstance(images_any, list) else []\n",
    "        return len(images)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def run_pipeline() -> Dict[str, int]:\n",
    "    # 입력 유효성 확인\n",
    "    if not TRAIN_LABEL_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Missing train labels: {str(TRAIN_LABEL_DIR)}\")\n",
    "    if not TRAIN_IMAGE_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Missing train images: {str(TRAIN_IMAGE_DIR)}\")\n",
    "    if not VAL_LABEL_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Missing val labels: {str(VAL_LABEL_DIR)}\")\n",
    "    if not VAL_IMAGE_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Missing val images: {str(VAL_IMAGE_DIR)}\")\n",
    "\n",
    "    ensure_dir(OUT_ROOT)\n",
    "\n",
    "    # 카메라 인덱스 각각 구축\n",
    "    train_cam_index: Dict[str, List[Path]] = build_camera_index(TRAIN_IMAGE_DIR)\n",
    "    val_cam_index: Dict[str, List[Path]] = build_camera_index(VAL_IMAGE_DIR)\n",
    "\n",
    "    # 카테고리 맵은 train 첫 JSON에서 추출\n",
    "    train_json_files: List[Path] = sorted(TRAIN_LABEL_DIR.rglob(\"*.json\"))\n",
    "    if len(train_json_files) == 0:\n",
    "        raise FileNotFoundError(f\"No train JSON files under {str(TRAIN_LABEL_DIR)}\")\n",
    "    id_to_idx: Dict[int, int]\n",
    "    id_to_name: Dict[int, str]\n",
    "    id_to_idx, id_to_name = build_category_maps(train_json_files[0])\n",
    "\n",
    "    total: Dict[str, int] = {\"copied\": 0, \"labeled\": 0, \"missing_image\": 0, \"skipped_stage\": 0}\n",
    "\n",
    "    # 진행률 합계를 위해 JSON별 이미지 개수를 미리 계산\n",
    "    train_img_counts: Dict[Path, int] = {jp: _count_images_in_json(jp) for jp in train_json_files}\n",
    "    total_train_imgs: int = sum(train_img_counts.values())\n",
    "\n",
    "    # 1) train 처리 진행바\n",
    "    with tqdm(total=len(train_json_files), desc=\"train json files\", unit=\"file\") as pbar_files, \\\n",
    "         tqdm(total=total_train_imgs, desc=\"train images\", unit=\"img\") as pbar_imgs:\n",
    "        jp: Path\n",
    "        for jp in train_json_files:\n",
    "            # 파일 처리\n",
    "            c: Dict[str, int] = process_coco_json_fixed_subset(\n",
    "                json_path=jp,\n",
    "                cam_index=train_cam_index,\n",
    "                out_root=OUT_ROOT,\n",
    "                stages=STAGES,\n",
    "                id_to_idx=id_to_idx,\n",
    "                id_to_name=id_to_name,\n",
    "                overwrite_labels=OVERWRITE_LABELS,\n",
    "                copy_images=COPY_IMAGES,\n",
    "                dry_run=DRY_RUN,\n",
    "                subset_name=\"train\"\n",
    "            )\n",
    "            # 카운트 갱신\n",
    "            k: str\n",
    "            for k in total.keys():\n",
    "                total[k] += int(c.get(k, 0))\n",
    "            # 진행바 업데이트\n",
    "            pbar_files.update(1)\n",
    "            pbar_imgs.update(train_img_counts.get(jp, 0))\n",
    "            pbar_files.set_postfix({\"labeled\": total[\"labeled\"], \"missing\": total[\"missing_image\"], \"skipped\": total[\"skipped_stage\"]})\n",
    "\n",
    "    # 2) val 처리 준비 및 진행바\n",
    "    val_json_files: List[Path] = sorted(VAL_LABEL_DIR.rglob(\"*.json\"))\n",
    "    if len(val_json_files) == 0:\n",
    "        raise FileNotFoundError(f\"No val JSON files under {str(VAL_LABEL_DIR)}\")\n",
    "    val_img_counts: Dict[Path, int] = {jp: _count_images_in_json(jp) for jp in val_json_files}\n",
    "    total_val_imgs: int = sum(val_img_counts.values())\n",
    "\n",
    "    with tqdm(total=len(val_json_files), desc=\"val json files\", unit=\"file\") as pbar_files, \\\n",
    "         tqdm(total=total_val_imgs, desc=\"val images\", unit=\"img\") as pbar_imgs:\n",
    "        jp = Path()  # 타입 명시 목적\n",
    "        for jp in val_json_files:\n",
    "            c = process_coco_json_fixed_subset(\n",
    "                json_path=jp,\n",
    "                cam_index=val_cam_index,\n",
    "                out_root=OUT_ROOT,\n",
    "                stages=STAGES,\n",
    "                id_to_idx=id_to_idx,\n",
    "                id_to_name=id_to_name,\n",
    "                overwrite_labels=OVERWRITE_LABELS,\n",
    "                copy_images=COPY_IMAGES,\n",
    "                dry_run=DRY_RUN,\n",
    "                subset_name=\"val\"\n",
    "            )\n",
    "            for k in total.keys():\n",
    "                total[k] += int(c.get(k, 0))\n",
    "            pbar_files.update(1)\n",
    "            pbar_imgs.update(val_img_counts.get(jp, 0))\n",
    "            pbar_files.set_postfix({\"labeled\": total[\"labeled\"], \"missing\": total[\"missing_image\"], \"skipped\": total[\"skipped_stage\"]})\n",
    "\n",
    "    # data.yaml 생성\n",
    "    ordered_items: List[Tuple[int, str]] = sorted(id_to_name.items(), key=lambda kv: kv[0])\n",
    "    idx_names: List[str] = [name for _, name in ordered_items]\n",
    "    stage_out: str\n",
    "    for stage_out in STAGES.keys():\n",
    "        yaml_path: Path = OUT_ROOT / stage_out / \"data.yaml\"\n",
    "        yaml_text: str = (\n",
    "            f\"path: {OUT_ROOT / stage_out}\\n\"\n",
    "            f\"train: train/images\\n\"\n",
    "            f\"val: val/images\\n\"\n",
    "            f\"nc: {len(idx_names)}\\n\"\n",
    "            f\"names: {idx_names}\\n\"\n",
    "        )\n",
    "        with open(yaml_path, \"w\", encoding=\"utf-8\") as yf:\n",
    "            yf.write(yaml_text)\n",
    "\n",
    "    return total\n",
    "\n",
    "\n",
    "counts_summary: Dict[str, int] = run_pipeline()\n",
    "counts_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be9a5c16-2086-4619-98dd-b96ecf0d2f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "copied -> 168318\n",
      "labeled -> 168318\n",
      "missing_image -> 4\n",
      "skipped_stage -> 0\n"
     ]
    }
   ],
   "source": [
    "summary: Dict[str, int] = counts_summary\n",
    "print(\"Done\")\n",
    "print(f\"copied -> {summary.get('copied', 0)}\")\n",
    "print(f\"labeled -> {summary.get('labeled', 0)}\")\n",
    "print(f\"missing_image -> {summary.get('missing_image', 0)}\")\n",
    "print(f\"skipped_stage -> {summary.get('skipped_stage', 0)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
