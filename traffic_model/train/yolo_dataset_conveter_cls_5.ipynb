{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816a77e4-9c6a-4510-ada0-f9f175e5f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "from glob import glob\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a17b158-9d3a-40be-92f6-1205db3645a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습(train) 세트 경로\n",
    "TRAIN_ROOT: Path = Path(\"dataset/traffic_data/training/bbox_tr\")\n",
    "TRAIN_LABEL_DIR: Path = TRAIN_ROOT / \"label_bbox\"\n",
    "TRAIN_IMAGE_DIR: Path = TRAIN_ROOT / \"image_bbox\"\n",
    "\n",
    "# 검증(val) 세트 경로\n",
    "VAL_ROOT: Path = Path(\"dataset/traffic_data/validation/bbox_val\")\n",
    "VAL_LABEL_DIR: Path = VAL_ROOT / \"label_bbox\"\n",
    "VAL_IMAGE_DIR: Path = VAL_ROOT / \"image_bbox\"\n",
    "\n",
    "# 출력 루트\n",
    "OUT_ROOT: Path = Path(\"yolo_transfer_cls_5\")\n",
    "\n",
    "# 전이학습 3단계 규칙 -> 정규화 없이 값 그대로 비교\n",
    "STAGES: Dict[str, Dict[str, Set[str]]] = {\n",
    "    \"day_data\": {\n",
    "        \"weathers\": set([\"Sunny\"]),\n",
    "        \"times\": set([\"낮\", \"오전\"])\n",
    "    },\n",
    "    \"night_data\": {\n",
    "        \"weathers\": set([\"Sunny\"]),\n",
    "        \"times\": set([\"새벽\", \"오후\"])\n",
    "    },\n",
    "    \"bad_weather_data\": {\n",
    "        \"weathers\": set([\"Cloudy\", \"Foggy\", \"Rainy\", \"Snow\"]),\n",
    "        \"times\": set([\"낮\", \"오전\", \"새벽\", \"오후\"])\n",
    "    }\n",
    "}\n",
    "\n",
    "# 실행 옵션\n",
    "OVERWRITE_LABELS: bool = True\n",
    "COPY_IMAGES: bool = True\n",
    "DRY_RUN: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c39330d-98b7-41ca-802d-180e07ac583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 후처리 규칙 추가\n",
    "# -> 최종 타깃 클래스 순서 고정\n",
    "TARGET_CLASS_ORDER: List[str] = [\n",
    "    \"승용차\",\n",
    "    \"버스\",\n",
    "    \"트럭\",\n",
    "    \"오토바이(자전거)\",\n",
    "    \"분류없음\"\n",
    "]\n",
    "\n",
    "# -> 병합 규칙: 원본 이름 -> 통합 이름\n",
    "MERGE_TO: Dict[str, str] = {\n",
    "    \"소형버스\": \"버스\",\n",
    "    \"대형버스\": \"버스\",\n",
    "}\n",
    "\n",
    "# -> 제거할 클래스 이름 집합\n",
    "DROP_SET: Set[str] = {\n",
    "    \"대형 트레일러\",     # 공백 포함 표기\n",
    "    \"대형트레일러\",       # 공백 없는 표기 가능성\n",
    "    \"보행자\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc60fb0-c3a9-410d-b2d1-bc3fbae69d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_camera_index(images_root: Path) -> Dict[str, List[Path]]:\n",
    "    # 패턴 일반화 -> image_bbox/site1/site2/camera_id/ 까지 모든 디렉터리 순회\n",
    "    cam_index: Dict[str, List[Path]] = {}\n",
    "    if not images_root.exists():\n",
    "        return cam_index\n",
    "    site1: Path\n",
    "    for site1 in images_root.iterdir():\n",
    "        if not site1.is_dir():\n",
    "            continue\n",
    "        site2: Path\n",
    "        for site2 in site1.iterdir():\n",
    "            if not site2.is_dir():\n",
    "                continue\n",
    "            cam_dir: Path\n",
    "            for cam_dir in site2.iterdir():\n",
    "                if not cam_dir.is_dir():\n",
    "                    continue\n",
    "                cam_id: str = cam_dir.name\n",
    "                cam_index.setdefault(cam_id, []).append(cam_dir)\n",
    "    return cam_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4366955-b38a-4d0d-bd5c-8d102e2816cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_path_from_index(cam_index: Dict[str, List[Path]], camera_id: str, filename: str) -> Optional[Path]:\n",
    "    dirs: List[Path] = cam_index.get(camera_id, [])\n",
    "    d: Path\n",
    "    for d in dirs:\n",
    "        candidate: Path = d / filename\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918fb3eb-dbdd-48f5-b0de-2e806965deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy_to_yolo(x1: float, y1: float, x2: float, y2: float, w: int, h: int) -> Tuple[float, float, float, float]:\n",
    "    x1c: float = float(min(max(x1, 0.0), float(w)))\n",
    "    y1c: float = float(min(max(y1, 0.0), float(h)))\n",
    "    x2c: float = float(min(max(x2, 0.0), float(w)))\n",
    "    y2c: float = float(min(max(y2, 0.0), float(h)))\n",
    "\n",
    "    xmin: float = float(min(x1c, x2c))\n",
    "    xmax: float = float(max(x1c, x2c))\n",
    "    ymin: float = float(min(y1c, y2c))\n",
    "    ymax: float = float(max(y1c, y2c))\n",
    "\n",
    "    bw: float = float(max(xmax - xmin, 1e-6))\n",
    "    bh: float = float(max(ymax - ymin, 1e-6))\n",
    "\n",
    "    cx: float = float(xmin + bw / 2.0)\n",
    "    cy: float = float(ymin + bh / 2.0)\n",
    "\n",
    "    x_norm: float = float(cx / float(w))\n",
    "    y_norm: float = float(cy / float(h))\n",
    "    w_norm: float = float(bw / float(w))\n",
    "    h_norm: float = float(bh / float(h))\n",
    "    return x_norm, y_norm, w_norm, h_norm\n",
    "\n",
    "def decide_stage(weather: str, time_space: str, stages: Dict[str, Dict[str, Set[str]]]) -> Optional[str]:\n",
    "    stage_name: str\n",
    "    rule: Dict[str, Set[str]]\n",
    "    for stage_name, rule in stages.items():\n",
    "        if weather in rule[\"weathers\"] and time_space in rule[\"times\"]:\n",
    "            return stage_name\n",
    "    return None\n",
    "\n",
    "def process_coco_json_fixed_subset(\n",
    "    json_path: Path,\n",
    "    cam_index: Dict[str, List[Path]],\n",
    "    out_root: Path,\n",
    "    stages: Dict[str, Dict[str, Set[str]]],\n",
    "    id_to_idx: Dict[int, int],\n",
    "    id_to_name: Dict[int, str],\n",
    "    overwrite_labels: bool,\n",
    "    copy_images: bool,\n",
    "    dry_run: bool,\n",
    "    subset_name: str\n",
    ") -> Dict[str, int]:\n",
    "    counts: Dict[str, int] = {\"copied\": 0, \"labeled\": 0, \"missing_image\": 0, \"skipped_stage\": 0}\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        coco_any: Any = json.load(f)\n",
    "\n",
    "    meta_list: List[Dict[str, Any]] = list(coco_any.get(\"meta\", []))  # type: ignore\n",
    "    meta_by_id: Dict[int, Dict[str, Any]] = {int(m.get(\"id\", -1)): m for m in meta_list}\n",
    "\n",
    "    images: List[Dict[str, Any]] = list(coco_any.get(\"images\", []))  # type: ignore\n",
    "    anns: List[Dict[str, Any]] = list(coco_any.get(\"annotations\", []))  # type: ignore\n",
    "\n",
    "    anns_by_image: Dict[int, List[Dict[str, Any]]] = {}\n",
    "    ann: Dict[str, Any]\n",
    "    for ann in anns:\n",
    "        img_id_val: int = int(ann.get(\"image_id\", -1))\n",
    "        anns_by_image.setdefault(img_id_val, []).append(ann)\n",
    "\n",
    "    stage_key: str\n",
    "    for stage_key in stages.keys():\n",
    "        ensure_dir(out_root / stage_key / subset_name / \"images\")\n",
    "        ensure_dir(out_root / stage_key / subset_name / \"labels\")\n",
    "\n",
    "    img: Dict[str, Any]\n",
    "    for img in images:\n",
    "        img_id: int = int(img.get(\"id\", -1))\n",
    "        meta_id: int = int(img.get(\"meta_id\", -1))\n",
    "        file_name: str = str(img.get(\"file_name\", \"\"))  # 예: \"BC2000201/xxxx.jpg\"\n",
    "\n",
    "        parts: List[str] = file_name.split(\"/\")\n",
    "        camera_id: str = parts[0] if len(parts) >= 2 else \"\"\n",
    "        fname: str = parts[-1]\n",
    "\n",
    "        meta: Dict[str, Any] = meta_by_id.get(meta_id, {})\n",
    "        weather: str = str(meta.get(\"weather\", \"\")).strip()\n",
    "        time_space: str = str(meta.get(\"time_space\", \"\")).strip()\n",
    "\n",
    "        stage: Optional[str] = decide_stage(weather, time_space, stages)\n",
    "        if stage is None:\n",
    "            counts[\"skipped_stage\"] += 1\n",
    "            continue\n",
    "\n",
    "        src_path_opt: Optional[Path] = find_image_path_from_index(cam_index, camera_id, fname)\n",
    "        if src_path_opt is None or not src_path_opt.exists():\n",
    "            counts[\"missing_image\"] += 1\n",
    "            continue\n",
    "        src_path: Path = src_path_opt\n",
    "\n",
    "        dst_img_dir: Path = out_root / stage / subset_name / \"images\" / camera_id\n",
    "        dst_lbl_dir: Path = out_root / stage / subset_name / \"labels\" / camera_id\n",
    "        ensure_dir(dst_img_dir)\n",
    "        ensure_dir(dst_lbl_dir)\n",
    "\n",
    "        dst_img_path: Path = dst_img_dir / fname\n",
    "        dst_lbl_path: Path = dst_lbl_dir / (Path(fname).stem + \".txt\")\n",
    "\n",
    "        with Image.open(src_path) as im:\n",
    "            size_tuple: Tuple[int, int] = im.size\n",
    "        w: int = int(size_tuple[0])\n",
    "        h: int = int(size_tuple[1])\n",
    "\n",
    "        lines: List[str] = []\n",
    "        img_anns: List[Dict[str, Any]] = anns_by_image.get(img_id, [])\n",
    "        ann_item: Dict[str, Any]\n",
    "        for ann_item in img_anns:\n",
    "            bboxes: List[List[float]] = [list(map(float, b)) for b in ann_item.get(\"bbox\", [])]  # type: ignore\n",
    "            cat_ids: List[int] = [int(c) for c in ann_item.get(\"category_id\", [])]  # type: ignore\n",
    "            idx: int\n",
    "            for idx in range(0, min(len(bboxes), len(cat_ids))):\n",
    "                b: List[float] = bboxes[idx]\n",
    "                c: int = cat_ids[idx]\n",
    "                if len(b) < 4:\n",
    "                    continue\n",
    "\n",
    "                # -> 여기서 필터링: id_to_idx에 없는 카테고리는 제거됨\n",
    "                if c not in id_to_idx:\n",
    "                    continue\n",
    "\n",
    "                x1: float = float(b[0]); y1: float = float(b[1]); x2: float = float(b[2]); y2: float = float(b[3])\n",
    "                cxn: float; cyn: float; wn: float; hn: float\n",
    "                cxn, cyn, wn, hn = xyxy_to_yolo(x1, y1, x2, y2, w, h)\n",
    "\n",
    "                cls_idx: int = int(id_to_idx[c])\n",
    "                line: str = f\"{cls_idx} {cxn:.6f} {cyn:.6f} {wn:.6f} {hn:.6f}\"\n",
    "                lines.append(line)\n",
    "\n",
    "        if not DRY_RUN:\n",
    "            if OVERWRITE_LABELS or not dst_lbl_path.exists():\n",
    "                with open(dst_lbl_path, \"w\", encoding=\"utf-8\") as lf:\n",
    "                    content: str = \"\\n\".join(lines)\n",
    "                    lf.write(content)\n",
    "            if COPY_IMAGES and not dst_img_path.exists():\n",
    "                shutil.copy2(src_path, dst_img_path)\n",
    "            counts[\"labeled\"] += 1\n",
    "            if COPY_IMAGES:\n",
    "                counts[\"copied\"] += 1\n",
    "        else:\n",
    "            counts[\"labeled\"] += 1\n",
    "            if COPY_IMAGES:\n",
    "                counts[\"copied\"] += 1\n",
    "\n",
    "    # data.yaml은 run_pipeline에서 한 번만 생성\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a6161b0-602e-4ea9-8cb8-34e4bcc418ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_category_maps(first_json_path: Path) -> Tuple[Dict[int, int], Dict[int, str]]:\n",
    "    # -> 원본 카테고리를 읽고, 병합/제거를 반영한 새 인덱스 매핑을 만든다.\n",
    "    with open(first_json_path, \"r\", encoding=\"utf-8\") as f0:\n",
    "        first_any: Any = json.load(f0)\n",
    "    categories: List[Dict[str, Any]] = list(first_any.get(\"categories\", []))  # type: ignore\n",
    "\n",
    "    # 원본 이름 -> 원본 id 매핑\n",
    "    name_to_id: Dict[str, int] = {}\n",
    "    cat: Dict[str, Any]\n",
    "    for cat in categories:\n",
    "        cid: int = int(cat.get(\"id\", 0))\n",
    "        cname: str = str(cat.get(\"name\", \"\"))\n",
    "        name_to_id[cname] = cid\n",
    "\n",
    "    # 최종 타깃 인덱스 정의 -> TARGET_CLASS_ORDER의 인덱스가 곧 최종 클래스 index\n",
    "    target_index_of: Dict[str, int] = {name: i for i, name in enumerate(TARGET_CLASS_ORDER)}\n",
    "\n",
    "    # 원본 id -> 최종 index 매핑\n",
    "    id_to_idx: Dict[int, int] = {}\n",
    "\n",
    "    # 모든 원본 카테고리에 대해 새 인덱스를 부여\n",
    "    cname: str\n",
    "    for cname, cid in name_to_id.items():\n",
    "        # 제거 대상이면 건너뜀\n",
    "        if cname in DROP_SET:\n",
    "            continue\n",
    "\n",
    "        # 병합 규칙 적용 -> 최종 이름\n",
    "        final_name: str = MERGE_TO.get(cname, cname)\n",
    "\n",
    "        # 최종 이름이 타깃 목록에 있어야 사용\n",
    "        if final_name not in target_index_of:\n",
    "            continue\n",
    "\n",
    "        new_idx: int = int(target_index_of[final_name])\n",
    "        id_to_idx[cid] = new_idx\n",
    "\n",
    "    # id_to_name 반환은 \"최종 index -> 최종 이름\" 매핑으로 구성\n",
    "    # -> run_pipeline의 data.yaml 생성 구간이 여기 반환값을 그대로 사용한다.\n",
    "    id_to_name: Dict[int, str] = {idx: name for name, idx in target_index_of.items()}\n",
    "\n",
    "    return id_to_idx, id_to_name\n",
    "\n",
    "\n",
    "def _count_images_in_json(json_path: Path) -> int:\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            coco_any: Any = json.load(f)\n",
    "        images_any: Any = coco_any.get(\"images\", [])\n",
    "        images: List[Dict[str, Any]] = list(images_any) if isinstance(images_any, list) else []\n",
    "        return len(images)\n",
    "    except Exception:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab92038b-37ca-41a0-b8f2-c3c28585d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline() -> Dict[str, int]:\n",
    "    # 입력 유효성 확인\n",
    "    if not TRAIN_LABEL_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Missing train labels: {str(TRAIN_LABEL_DIR)}\")\n",
    "    if not TRAIN_IMAGE_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Missing train images: {str(TRAIN_IMAGE_DIR)}\")\n",
    "    if not VAL_LABEL_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Missing val labels: {str(VAL_LABEL_DIR)}\")\n",
    "    if not VAL_IMAGE_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Missing val images: {str(VAL_IMAGE_DIR)}\")\n",
    "\n",
    "    ensure_dir(OUT_ROOT)\n",
    "\n",
    "    # 카메라 인덱스 각각 구축\n",
    "    train_cam_index: Dict[str, List[Path]] = build_camera_index(TRAIN_IMAGE_DIR)\n",
    "    val_cam_index: Dict[str, List[Path]] = build_camera_index(VAL_IMAGE_DIR)\n",
    "\n",
    "    # 카테고리 맵은 train 첫 JSON에서 추출\n",
    "    train_json_files: List[Path] = sorted(TRAIN_LABEL_DIR.rglob(\"*.json\"))\n",
    "    if len(train_json_files) == 0:\n",
    "        raise FileNotFoundError(f\"No train JSON files under {str(TRAIN_LABEL_DIR)}\")\n",
    "    id_to_idx: Dict[int, int]\n",
    "    id_to_name: Dict[int, str]\n",
    "    id_to_idx, id_to_name = build_category_maps(train_json_files[0])\n",
    "\n",
    "    total: Dict[str, int] = {\"copied\": 0, \"labeled\": 0, \"missing_image\": 0, \"skipped_stage\": 0}\n",
    "\n",
    "    # 진행률 합계를 위해 JSON별 이미지 개수를 미리 계산\n",
    "    train_img_counts: Dict[Path, int] = {jp: _count_images_in_json(jp) for jp in train_json_files}\n",
    "    total_train_imgs: int = sum(train_img_counts.values())\n",
    "\n",
    "    # 1) train 처리 진행바\n",
    "    with tqdm(total=len(train_json_files), desc=\"train json files\", unit=\"file\") as pbar_files, \\\n",
    "         tqdm(total=total_train_imgs, desc=\"train images\", unit=\"img\") as pbar_imgs:\n",
    "        jp: Path\n",
    "        for jp in train_json_files:\n",
    "            # 파일 처리\n",
    "            c: Dict[str, int] = process_coco_json_fixed_subset(\n",
    "                json_path=jp,\n",
    "                cam_index=train_cam_index,\n",
    "                out_root=OUT_ROOT,\n",
    "                stages=STAGES,\n",
    "                id_to_idx=id_to_idx,\n",
    "                id_to_name=id_to_name,\n",
    "                overwrite_labels=OVERWRITE_LABELS,\n",
    "                copy_images=COPY_IMAGES,\n",
    "                dry_run=DRY_RUN,\n",
    "                subset_name=\"train\"\n",
    "            )\n",
    "            # 카운트 갱신\n",
    "            k: str\n",
    "            for k in total.keys():\n",
    "                total[k] += int(c.get(k, 0))\n",
    "            # 진행바 업데이트\n",
    "            pbar_files.update(1)\n",
    "            pbar_imgs.update(train_img_counts.get(jp, 0))\n",
    "            pbar_files.set_postfix({\"labeled\": total[\"labeled\"], \"missing\": total[\"missing_image\"], \"skipped\": total[\"skipped_stage\"]})\n",
    "\n",
    "    # 2) val 처리 준비 및 진행바\n",
    "    val_json_files: List[Path] = sorted(VAL_LABEL_DIR.rglob(\"*.json\"))\n",
    "    if len(val_json_files) == 0:\n",
    "        raise FileNotFoundError(f\"No val JSON files under {str(VAL_LABEL_DIR)}\")\n",
    "    val_img_counts: Dict[Path, int] = {jp: _count_images_in_json(jp) for jp in val_json_files}\n",
    "    total_val_imgs: int = sum(val_img_counts.values())\n",
    "\n",
    "    with tqdm(total=len(val_json_files), desc=\"val json files\", unit=\"file\") as pbar_files, \\\n",
    "         tqdm(total=total_val_imgs, desc=\"val images\", unit=\"img\") as pbar_imgs:\n",
    "        jp = Path()  # 타입 명시 목적\n",
    "        for jp in val_json_files:\n",
    "            c = process_coco_json_fixed_subset(\n",
    "                json_path=jp,\n",
    "                cam_index=val_cam_index,\n",
    "                out_root=OUT_ROOT,\n",
    "                stages=STAGES,\n",
    "                id_to_idx=id_to_idx,\n",
    "                id_to_name=id_to_name,\n",
    "                overwrite_labels=OVERWRITE_LABELS,\n",
    "                copy_images=COPY_IMAGES,\n",
    "                dry_run=DRY_RUN,\n",
    "                subset_name=\"val\"\n",
    "            )\n",
    "            for k in total.keys():\n",
    "                total[k] += int(c.get(k, 0))\n",
    "            pbar_files.update(1)\n",
    "            pbar_imgs.update(val_img_counts.get(jp, 0))\n",
    "            pbar_files.set_postfix({\"labeled\": total[\"labeled\"], \"missing\": total[\"missing_image\"], \"skipped\": total[\"skipped_stage\"]})\n",
    "\n",
    "    # data.yaml 생성 -> build_category_maps에서 반환한 \"최종 index -> 이름\"을 사용\n",
    "    ordered_items: List[Tuple[int, str]] = sorted(id_to_name.items(), key=lambda kv: kv[0])\n",
    "    idx_names: List[str] = [name for _, name in ordered_items]\n",
    "    stage_out: str\n",
    "    for stage_out in STAGES.keys():\n",
    "        yaml_path: Path = OUT_ROOT / stage_out / \"data.yaml\"\n",
    "        yaml_text: str = (\n",
    "            f\"path: {OUT_ROOT / stage_out}\\n\"\n",
    "            f\"train: train/images\\n\"\n",
    "            f\"val: val/images\\n\"\n",
    "            f\"nc: {len(idx_names)}\\n\"\n",
    "            f\"names: {idx_names}\\n\"\n",
    "        )\n",
    "        with open(yaml_path, \"w\", encoding=\"utf-8\") as yf:\n",
    "            yf.write(yaml_text)\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d906df9-bbe4-4a3d-aacc-2894f4f6f18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train json files:   0%|          | 0/72 [00:00<?, ?file/s]\n",
      "train json files:   1%|▏         | 1/72 [00:20<24:06, 20.38s/file]\n",
      "train json files:   3%|▎         | 2/72 [00:38<22:06, 18.95s/file, labeled=2404, missing=0, skipped=0]\n",
      "train json files:   3%|▎         | 2/72 [00:38<22:06, 18.95s/file, labeled=4808, missing=0, skipped=0]\n",
      "train json files:   4%|▍         | 3/72 [00:54<20:08, 17.52s/file, labeled=4808, missing=0, skipped=0]\n",
      "train json files:   6%|▌         | 4/72 [01:07<17:51, 15.75s/file, labeled=6590, missing=0, skipped=0]\n",
      "train json files:   7%|▋         | 5/72 [01:11<12:48, 11.47s/file, labeled=8372, missing=0, skipped=0]\n",
      "train json files:   8%|▊         | 6/72 [01:25<13:36, 12.37s/file, labeled=8865, missing=0, skipped=0]\n",
      "train json files:  10%|▉         | 7/72 [01:28<10:15,  9.47s/file, labeled=10391, missing=0, skipped=0]\n",
      "train json files:  10%|▉         | 7/72 [01:28<10:15,  9.47s/file, labeled=10818, missing=0, skipped=0]\n",
      "train json files:  11%|█         | 8/72 [02:34<29:23, 27.56s/file, labeled=10818, missing=0, skipped=0]\n",
      "train json files:  12%|█▎        | 9/72 [02:49<24:32, 23.38s/file, labeled=17882, missing=0, skipped=0]\n",
      "train json files:  12%|█▎        | 9/72 [02:49<24:32, 23.38s/file, labeled=19388, missing=0, skipped=0]\n",
      "train json files:  14%|█▍        | 10/72 [03:06<22:18, 21.60s/file, labeled=19388, missing=0, skipped=0]\n",
      "train json files:  14%|█▍        | 10/72 [03:06<22:18, 21.60s/file, labeled=21409, missing=0, skipped=0]\n",
      "train json files:  15%|█▌        | 11/72 [03:40<25:48, 25.38s/file, labeled=21409, missing=0, skipped=0]\n",
      "train json files:  17%|█▋        | 12/72 [03:43<18:27, 18.45s/file, labeled=25046, missing=0, skipped=0]\n",
      "train json files:  18%|█▊        | 13/72 [03:56<16:34, 16.86s/file, labeled=25346, missing=0, skipped=0]\n",
      "train json files:  18%|█▊        | 13/72 [03:56<16:34, 16.86s/file, labeled=26959, missing=0, skipped=0]\n",
      "train json files:  19%|█▉        | 14/72 [04:18<17:47, 18.40s/file, labeled=26959, missing=0, skipped=0]\n",
      "train json files:  19%|█▉        | 14/72 [04:18<17:47, 18.40s/file, labeled=29668, missing=0, skipped=0]\n",
      "train json files:  21%|██        | 15/72 [04:35<16:59, 17.88s/file, labeled=29668, missing=0, skipped=0]\n",
      "train json files:  21%|██        | 15/72 [04:35<16:59, 17.88s/file, labeled=31596, missing=0, skipped=0]\n",
      "train json files:  22%|██▏       | 16/72 [05:01<18:57, 20.32s/file, labeled=31596, missing=0, skipped=0]\n",
      "train json files:  24%|██▎       | 17/72 [05:18<17:49, 19.45s/file, labeled=34628, missing=0, skipped=0]\n",
      "train json files:  24%|██▎       | 17/72 [05:18<17:49, 19.45s/file, labeled=36674, missing=0, skipped=0]\n",
      "train json files:  25%|██▌       | 18/72 [05:58<23:00, 25.57s/file, labeled=36674, missing=0, skipped=0]\n",
      "train json files:  25%|██▌       | 18/72 [05:58<23:00, 25.57s/file, labeled=41084, missing=0, skipped=0]\n",
      "train json files:  26%|██▋       | 19/72 [06:15<20:20, 23.04s/file, labeled=41084, missing=0, skipped=0]\n",
      "train json files:  26%|██▋       | 19/72 [06:15<20:20, 23.04s/file, labeled=42945, missing=0, skipped=0]\n",
      "train json files:  28%|██▊       | 20/72 [06:44<21:35, 24.91s/file, labeled=42945, missing=0, skipped=0]\n",
      "train json files:  28%|██▊       | 20/72 [06:44<21:35, 24.91s/file, labeled=46126, missing=0, skipped=0]\n",
      "train json files:  29%|██▉       | 21/72 [07:03<19:36, 23.07s/file, labeled=46126, missing=0, skipped=0]\n",
      "train json files:  31%|███       | 22/72 [07:03<13:29, 16.18s/file, labeled=48161, missing=0, skipped=0]\n",
      "train json files:  32%|███▏      | 23/72 [07:03<13:12, 16.18s/file, labeled=48162, missing=0, skipped=0]\n",
      "train json files:  33%|███▎      | 24/72 [07:35<12:54, 16.14s/file, labeled=48162, missing=0, skipped=0]\n",
      "train json files:  33%|███▎      | 24/72 [07:35<12:54, 16.14s/file, labeled=51718, missing=0, skipped=0]\n",
      "train json files:  35%|███▍      | 25/72 [07:52<12:49, 16.38s/file, labeled=51718, missing=0, skipped=0]\n",
      "train json files:  35%|███▍      | 25/72 [07:52<12:49, 16.38s/file, labeled=53941, missing=0, skipped=0]\n",
      "train json files:  36%|███▌      | 26/72 [08:15<13:44, 17.92s/file, labeled=53941, missing=0, skipped=0]\n",
      "train json files:  36%|███▌      | 26/72 [08:15<13:44, 17.92s/file, labeled=56543, missing=0, skipped=0]\n",
      "train json files:  38%|███▊      | 27/72 [08:32<13:17, 17.72s/file, labeled=56543, missing=0, skipped=0]\n",
      "train json files:  38%|███▊      | 27/72 [08:32<13:17, 17.72s/file, labeled=58434, missing=0, skipped=0]\n",
      "train json files:  39%|███▉      | 28/72 [08:51<13:15, 18.08s/file, labeled=58434, missing=0, skipped=0]\n",
      "train json files:  39%|███▉      | 28/72 [08:51<13:15, 18.08s/file, labeled=60465, missing=0, skipped=0]\n",
      "train json files:  40%|████      | 29/72 [09:20<15:12, 21.22s/file, labeled=60465, missing=0, skipped=0]\n",
      "train json files:  40%|████      | 29/72 [09:20<15:12, 21.22s/file, labeled=63339, missing=0, skipped=0]\n",
      "train json files:  42%|████▏     | 30/72 [10:25<23:40, 33.83s/file, labeled=63339, missing=0, skipped=0]\n",
      "train json files:  43%|████▎     | 31/72 [10:35<18:26, 26.99s/file, labeled=69837, missing=0, skipped=0]\n",
      "train json files:  43%|████▎     | 31/72 [10:35<18:26, 26.99s/file, labeled=71062, missing=0, skipped=0]\n",
      "train json files:  44%|████▍     | 32/72 [10:55<16:29, 24.75s/file, labeled=71062, missing=0, skipped=0]\n",
      "train json files:  44%|████▍     | 32/72 [10:55<16:29, 24.75s/file, labeled=73139, missing=0, skipped=0]\n",
      "train json files:  46%|████▌     | 33/72 [11:21<16:24, 25.24s/file, labeled=73139, missing=0, skipped=0]\n",
      "train json files:  46%|████▌     | 33/72 [11:21<16:24, 25.24s/file, labeled=76231, missing=0, skipped=0]\n",
      "train json files:  47%|████▋     | 34/72 [11:56<17:50, 28.17s/file, labeled=76231, missing=0, skipped=0]\n",
      "train json files:  47%|████▋     | 34/72 [11:56<17:50, 28.17s/file, labeled=80164, missing=0, skipped=0]\n",
      "train json files:  49%|████▊     | 35/72 [12:24<17:15, 27.98s/file, labeled=80164, missing=0, skipped=0]\n",
      "train json files:  50%|█████     | 36/72 [12:34<13:38, 22.74s/file, labeled=83453, missing=0, skipped=0]\n",
      "train json files:  51%|█████▏    | 37/72 [12:42<10:36, 18.17s/file, labeled=84540, missing=0, skipped=0]\n",
      "train json files:  53%|█████▎    | 38/72 [12:52<09:00, 15.89s/file, labeled=85341, missing=1, skipped=0]\n",
      "train json files:  54%|█████▍    | 39/72 [13:08<08:41, 15.81s/file, labeled=86445, missing=1, skipped=0]\n",
      "train json files:  54%|█████▍    | 39/72 [13:08<08:41, 15.81s/file, labeled=88057, missing=1, skipped=0]\n",
      "train json files:  56%|█████▌    | 40/72 [13:33<09:55, 18.61s/file, labeled=88057, missing=1, skipped=0]\n",
      "train json files:  57%|█████▋    | 41/72 [13:36<07:14, 14.02s/file, labeled=90585, missing=1, skipped=0]\n",
      "train json files:  57%|█████▋    | 41/72 [13:36<07:14, 14.02s/file, labeled=91050, missing=1, skipped=0]\n",
      "train json files:  58%|█████▊    | 42/72 [13:59<08:15, 16.52s/file, labeled=91050, missing=1, skipped=0]\n",
      "train json files:  58%|█████▊    | 42/72 [13:59<08:15, 16.52s/file, labeled=93528, missing=1, skipped=0]\n",
      "train json files:  60%|█████▉    | 43/72 [14:12<07:36, 15.75s/file, labeled=93528, missing=1, skipped=0]\n",
      "train json files:  61%|██████    | 44/72 [14:25<06:51, 14.68s/file, labeled=95162, missing=1, skipped=0]\n",
      "train json files:  61%|██████    | 44/72 [14:25<06:51, 14.68s/file, labeled=96579, missing=1, skipped=0]\n",
      "train json files:  62%|██████▎   | 45/72 [14:48<07:46, 17.26s/file, labeled=96579, missing=1, skipped=0]\n",
      "train json files:  62%|██████▎   | 45/72 [14:48<07:46, 17.26s/file, labeled=99152, missing=1, skipped=0]\n",
      "train json files:  64%|██████▍   | 46/72 [15:03<07:09, 16.51s/file, labeled=99152, missing=1, skipped=0]\n",
      "train json files:  64%|██████▍   | 46/72 [15:03<07:09, 16.51s/file, labeled=100963, missing=1, skipped=0]\n",
      "train json files:  65%|██████▌   | 47/72 [15:29<08:08, 19.53s/file, labeled=100963, missing=1, skipped=0]\n",
      "train json files:  65%|██████▌   | 47/72 [15:29<08:08, 19.53s/file, labeled=103845, missing=1, skipped=0]\n",
      "train json files:  67%|██████▋   | 48/72 [16:04<09:35, 23.96s/file, labeled=103845, missing=1, skipped=0]\n",
      "train json files:  67%|██████▋   | 48/72 [16:04<09:35, 23.96s/file, labeled=107423, missing=1, skipped=0]\n",
      "train json files:  68%|██████▊   | 49/72 [16:30<09:25, 24.57s/file, labeled=107423, missing=1, skipped=0]\n",
      "train json files:  68%|██████▊   | 49/72 [16:30<09:25, 24.57s/file, labeled=110109, missing=1, skipped=0]\n",
      "train json files:  69%|██████▉   | 50/72 [17:17<11:33, 31.52s/file, labeled=110109, missing=1, skipped=0]\n",
      "train json files:  69%|██████▉   | 50/72 [17:17<11:33, 31.52s/file, labeled=115025, missing=1, skipped=0]\n",
      "train json files:  71%|███████   | 51/72 [17:57<11:54, 34.04s/file, labeled=115025, missing=1, skipped=0]\n",
      "train json files:  72%|███████▏  | 52/72 [18:07<08:57, 26.89s/file, labeled=119019, missing=1, skipped=0]\n",
      "train json files:  74%|███████▎  | 53/72 [18:11<06:15, 19.78s/file, labeled=120250, missing=1, skipped=0]\n",
      "train json files:  75%|███████▌  | 54/72 [18:19<04:56, 16.47s/file, labeled=120659, missing=1, skipped=0]\n",
      "train json files:  76%|███████▋  | 55/72 [18:24<03:41, 13.01s/file, labeled=121859, missing=1, skipped=0]\n",
      "train json files:  78%|███████▊  | 56/72 [18:30<02:53, 10.87s/file, labeled=122490, missing=1, skipped=0]\n",
      "train json files:  79%|███████▉  | 57/72 [18:43<02:53, 11.56s/file, labeled=123262, missing=1, skipped=0]\n",
      "train json files:  79%|███████▉  | 57/72 [18:43<02:53, 11.56s/file, labeled=124632, missing=1, skipped=0]\n",
      "train json files:  81%|████████  | 58/72 [19:22<04:35, 19.68s/file, labeled=124632, missing=1, skipped=0]\n",
      "train json files:  82%|████████▏ | 59/72 [19:32<03:37, 16.70s/file, labeled=128408, missing=1, skipped=0]\n",
      "train json files:  82%|████████▏ | 59/72 [19:32<03:37, 16.70s/file, labeled=129577, missing=1, skipped=0]\n",
      "train json files:  83%|████████▎ | 60/72 [20:02<04:09, 20.80s/file, labeled=129577, missing=1, skipped=0]\n",
      "train json files:  85%|████████▍ | 61/72 [20:14<03:20, 18.22s/file, labeled=132832, missing=3, skipped=0]\n",
      "train json files:  85%|████████▍ | 61/72 [20:14<03:20, 18.22s/file, labeled=134373, missing=3, skipped=0]\n",
      "train json files:  86%|████████▌ | 62/72 [20:37<03:14, 19.46s/file, labeled=134373, missing=3, skipped=0]\n",
      "train json files:  88%|████████▊ | 63/72 [20:45<02:23, 15.98s/file, labeled=137205, missing=3, skipped=0]\n",
      "train json files:  88%|████████▊ | 63/72 [20:45<02:23, 15.98s/file, labeled=138141, missing=3, skipped=0]\n",
      "train json files:  89%|████████▉ | 64/72 [21:03<02:13, 16.64s/file, labeled=138141, missing=3, skipped=0]\n",
      "train json files:  90%|█████████ | 65/72 [21:14<01:45, 15.02s/file, labeled=140312, missing=3, skipped=0]\n",
      "train json files:  90%|█████████ | 65/72 [21:14<01:45, 15.02s/file, labeled=141640, missing=3, skipped=0]\n",
      "train json files:  92%|█████████▏| 66/72 [21:43<01:54, 19.16s/file, labeled=141640, missing=3, skipped=0]\n",
      "train json files:  93%|█████████▎| 67/72 [21:47<01:13, 14.71s/file, labeled=144983, missing=3, skipped=0]\n",
      "train json files:  94%|█████████▍| 68/72 [21:48<00:42, 10.71s/file, labeled=145519, missing=3, skipped=0]\n",
      "train json files:  94%|█████████▍| 68/72 [21:48<00:42, 10.71s/file, labeled=145677, missing=3, skipped=0]\n",
      "train json files:  96%|█████████▌| 69/72 [22:11<00:42, 14.27s/file, labeled=145677, missing=3, skipped=0]\n",
      "train json files:  97%|█████████▋| 70/72 [22:14<00:21, 10.95s/file, labeled=148695, missing=3, skipped=0]\n",
      "train json files:  99%|█████████▊| 71/72 [22:16<00:08,  8.21s/file, labeled=149079, missing=3, skipped=0]\n",
      "train json files: 100%|██████████| 72/72 [22:22<00:00,  7.40s/file, labeled=149277, missing=3, skipped=0]\n",
      "train images: 100%|██████████| 149887/149887 [22:22<00:00, 111.68img/s]eled=149884, missing=3, skipped=0]\n",
      "train json files: 100%|██████████| 72/72 [22:22<00:00, 18.64s/file, labeled=149884, missing=3, skipped=0]\n",
      "val json files:   0%|          | 0/46 [00:00<?, ?file/s]\n",
      "val json files:   2%|▏         | 1/46 [00:02<01:36,  2.14s/file]\n",
      "val json files:   4%|▍         | 2/46 [00:03<01:23,  1.90s/file, labeled=150107, missing=3, skipped=0]\n",
      "val json files:   7%|▋         | 3/46 [00:04<01:06,  1.55s/file, labeled=150330, missing=3, skipped=0]\n",
      "val json files:   9%|▊         | 4/46 [00:07<01:23,  1.99s/file, labeled=150465, missing=3, skipped=0]\n",
      "val json files:  11%|█         | 5/46 [00:09<01:21,  1.98s/file, labeled=150758, missing=3, skipped=0]\n",
      "val json files:  13%|█▎        | 6/46 [00:13<01:40,  2.52s/file, labeled=150996, missing=3, skipped=0]\n",
      "val json files:  15%|█▌        | 7/46 [00:14<01:24,  2.16s/file, labeled=151393, missing=3, skipped=0]\n",
      "val json files:  17%|█▋        | 8/46 [00:18<01:45,  2.77s/file, labeled=151563, missing=3, skipped=0]\n",
      "val json files:  20%|█▉        | 9/46 [00:23<02:03,  3.35s/file, labeled=152007, missing=3, skipped=0]\n",
      "val json files:  22%|██▏       | 10/46 [00:24<01:38,  2.74s/file, labeled=152477, missing=3, skipped=0]\n",
      "val json files:  24%|██▍       | 11/46 [00:29<01:56,  3.33s/file, labeled=152632, missing=3, skipped=0]\n",
      "val json files:  26%|██▌       | 12/46 [00:36<02:31,  4.44s/file, labeled=153112, missing=3, skipped=0]\n",
      "val json files:  28%|██▊       | 13/46 [00:38<02:05,  3.79s/file, labeled=153910, missing=3, skipped=0]\n",
      "val json files:  30%|███       | 14/46 [00:41<01:50,  3.45s/file, labeled=154196, missing=3, skipped=0]\n",
      "val json files:  33%|███▎      | 15/46 [00:42<01:25,  2.77s/file, labeled=154528, missing=3, skipped=0]\n",
      "val json files:  35%|███▍      | 16/46 [00:48<01:49,  3.64s/file, labeled=154670, missing=3, skipped=0]\n",
      "val json files:  37%|███▋      | 17/46 [00:57<02:34,  5.32s/file, labeled=155258, missing=4, skipped=0]\n",
      "val json files:  37%|███▋      | 17/46 [00:57<02:34,  5.32s/file, labeled=156188, missing=4, skipped=0]\n",
      "val json files:  39%|███▉      | 18/46 [01:17<04:31,  9.70s/file, labeled=156188, missing=4, skipped=0]\n",
      "val json files:  41%|████▏     | 19/46 [01:20<03:28,  7.71s/file, labeled=158167, missing=4, skipped=0]\n",
      "val json files:  43%|████▎     | 20/46 [01:26<03:08,  7.24s/file, labeled=158536, missing=4, skipped=0]\n",
      "val json files:  46%|████▌     | 21/46 [01:28<02:19,  5.60s/file, labeled=159181, missing=4, skipped=0]\n",
      "val json files:  48%|████▊     | 22/46 [01:30<01:50,  4.62s/file, labeled=159406, missing=4, skipped=0]\n",
      "val json files:  50%|█████     | 23/46 [01:31<01:20,  3.50s/file, labeled=159668, missing=4, skipped=0]\n",
      "val json files:  52%|█████▏    | 24/46 [01:34<01:15,  3.42s/file, labeled=159765, missing=4, skipped=0]\n",
      "val json files:  54%|█████▍    | 25/46 [01:36<01:01,  2.94s/file, labeled=160114, missing=4, skipped=0]\n",
      "val json files:  57%|█████▋    | 26/46 [01:38<00:53,  2.65s/file, labeled=160278, missing=4, skipped=0]\n",
      "val json files:  59%|█████▊    | 27/46 [01:41<00:52,  2.76s/file, labeled=160498, missing=4, skipped=0]\n",
      "val json files:  61%|██████    | 28/46 [01:46<01:03,  3.53s/file, labeled=160809, missing=4, skipped=0]\n",
      "val json files:  63%|██████▎   | 29/46 [01:51<01:04,  3.78s/file, labeled=161338, missing=4, skipped=0]\n",
      "val json files:  65%|██████▌   | 30/46 [01:53<00:51,  3.20s/file, labeled=161844, missing=4, skipped=0]\n",
      "val json files:  67%|██████▋   | 31/46 [01:54<00:39,  2.66s/file, labeled=162071, missing=4, skipped=0]\n",
      "val json files:  70%|██████▉   | 32/46 [01:56<00:35,  2.54s/file, labeled=162238, missing=4, skipped=0]\n",
      "val json files:  72%|███████▏  | 33/46 [01:58<00:29,  2.28s/file, labeled=162465, missing=4, skipped=0]\n",
      "val json files:  74%|███████▍  | 34/46 [02:04<00:40,  3.34s/file, labeled=162664, missing=4, skipped=0]\n",
      "val json files:  76%|███████▌  | 35/46 [02:04<00:27,  2.54s/file, labeled=163269, missing=4, skipped=0]\n",
      "val json files:  78%|███████▊  | 36/46 [02:07<00:26,  2.64s/file, labeled=163352, missing=4, skipped=0]\n",
      "val json files:  80%|████████  | 37/46 [02:16<00:40,  4.50s/file, labeled=163635, missing=4, skipped=0]\n",
      "val json files:  83%|████████▎ | 38/46 [02:18<00:29,  3.65s/file, labeled=164546, missing=4, skipped=0]\n",
      "val json files:  85%|████████▍ | 39/46 [02:24<00:30,  4.38s/file, labeled=164729, missing=4, skipped=0]\n",
      "val json files:  87%|████████▋ | 40/46 [02:26<00:21,  3.60s/file, labeled=165376, missing=4, skipped=0]\n",
      "val json files:  89%|████████▉ | 41/46 [02:29<00:17,  3.46s/file, labeled=165613, missing=4, skipped=0]\n",
      "val json files:  91%|█████████▏| 42/46 [02:31<00:12,  3.08s/file, labeled=166038, missing=4, skipped=0]\n",
      "val json files:  93%|█████████▎| 43/46 [02:36<00:11,  3.78s/file, labeled=166316, missing=4, skipped=0]\n",
      "val json files:  96%|█████████▌| 44/46 [02:40<00:07,  3.85s/file, labeled=166948, missing=4, skipped=0]\n",
      "val json files:  98%|█████████▊| 45/46 [02:47<00:04,  4.73s/file, labeled=167433, missing=4, skipped=0]\n",
      "val json files: 100%|██████████| 46/46 [02:48<00:00,  3.48s/file, labeled=168250, missing=4, skipped=0]\n",
      "val images: 100%|██████████| 18435/18435 [02:48<00:00, 109.58img/s]abeled=168318, missing=4, skipped=0]\n",
      "val json files: 100%|██████████| 46/46 [02:48<00:00,  3.66s/file, labeled=168318, missing=4, skipped=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "copied -> 168318\n",
      "labeled -> 168318\n",
      "missing_image -> 4\n",
      "skipped_stage -> 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "counts_summary: Dict[str, int] = run_pipeline()\n",
    "counts_summary\n",
    "\n",
    "summary: Dict[str, int] = counts_summary\n",
    "\n",
    "print(\"Done\")\n",
    "print(f\"copied -> {summary.get('copied', 0)}\")\n",
    "print(f\"labeled -> {summary.get('labeled', 0)}\")\n",
    "print(f\"missing_image -> {summary.get('missing_image', 0)}\")\n",
    "print(f\"skipped_stage -> {summary.get('skipped_stage', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea71c6-e99c-4811-ab1c-8943f935c0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
